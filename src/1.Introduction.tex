
Parallel computing is an area of academic focus. The interest is two-fold. Firstly, given their role, educators adapt to trends in research and industry and include parallel computing in their curricula, and secondly, parallel computing is an attractive field for department research and industry collaborations. 

While industry-driven collaborations often predefine technological frameworks such as programming environments and target architectures, academic teaching leaves this mostly undefined. In this case, educators can select from a variety of architectures and programming models for their parallel computing curricula. Which particular set to choose for teaching in class is not trivial. While decision factors differ, maximizing the subject relevance by teaching generic principles of parallel programming is among the most important ones. 

Consequently, generic parallel programming models are desirable that target mainstream programming languages and allow a vendor- and architecture-independent, performance-portable expression of concurrency. In this way, a parallel program becomes capable of running efficiently on computer architectures that conform to a common parallel machine specification. We would like to compare this to the support of Von Neumann architectures in sequential programming languages today. 

While message-passing interfaces, OpenMP, OpenCL or CUDA maintain their role in teaching, the plethora of novel parallel programming models~\cite{Chapel}~\cite{UPC++}~\cite{Raja} hints towards an emphasis on productivity in parallel programming and also brings up the valid question whether this strengthens the argument for their inclusion in education. 

Based on our experience in supercomputing, we see that productivity is gaining importance as the number of hardware solutions, their complexity to program as well as demands for a quick time-to-solution increase. Consequently, we think that the inclusion of novel programming models is worth consideration in case they offer generic programming concepts for a vendor- and architecture-independent, performance-portable expression of concurrency and can contribute to parallel programming productivity. 

It is the productivity concern as well as the need for a generic expression of concurrency due to periodic changes of supercomputer architectures and vendors in national projects that inspired the development of Kokkos. The increasing adoption of Kokkos in these projects confirms its usefulness which motivates us to present Kokkos to academia in this work.

Kokkos\footnote{from Greek for grain or seed}~\cite{KOKKOS} is a performance-portable, parallel programming model for C++ developed collaboratively by several national laboratories as part of the US Department of Energy Exascale Project\cite{ECP}. It consists of a programming model specification and a library. The specification defines the execution and memory model and an application programming interface (library API). The runtime library exposes programming primitives, namely execution patterns and types, including views and execution spaces and maps them to the underlying abstract machine model. The machine model creates a generic view of hardware and memories. Finally, the Kokkos library implements the mapping of the programming model to vendor-specific programming models, libraries, and memory layouts. Kokkos has been successfully used in many large-scale projects at Sandia National Laboratories, Oak-Ridge National Laboratories, Los Alamos National Laboratories, University of Utah, and others~\cite{KOKKOSPROJS}. We list its key properties as follows.

\begin{itemize}
\item Targets an abstract machine model
\item Implements patterns, views, and spaces
\item Vendor- and architecture-independent and open-source
\item Aligns with C++ standardization efforts for parallel programming
\item Includes exercises and teaching material
\end{itemize}

The objective of this paper is to present Kokkos from three perspectives. Firstly, we would like to present our thoughts that accompanied the design and development of the Kokkos parallel programming model. These insights may be useful in parallel programming classes that introduce programming models, reason about design choices, and cover productivity in parallel software development.

Further, we would like to promote Kokkos to students and educators as a platform that helps them to understand and experiment with parallel patterns in a way that is generic, vendor-agnostic, and declarative. Abstractions allow students to learn about concurrency without being exposed to many architectural concerns initially and help to create awareness of their productivity benefits in terms of code- and performance portability on current and upcoming hardware architectures.

Finally, we would like to present Kokkos to the educational community as one representative parallel programming model for a trend towards portability abstractions in parallel programming models. We show a set of interfaces that implement patterns, views, and spaces, and capture enough semantic information to produce performance-portable, parallel code on current architectures. Further, we highlight its alignment and contributions to standardization efforts towards supporting parallel programming in C++. For teaching insights, we document our best practices obtained from teaching Kokkos as well as feedback received during on-line lectures.

The rest of this paper is structured as follows. The next chapter gives an overview of parallel programming concepts and presents the notion of \emph{semantic capture}. Chapter~\ref{chap:kokkosMM} presents the abstract machine model used to derive the required semantic information for performance portability in Kokkos. Chapters~\ref{chap:kokkosPM} and ~\ref{chap:kokkosExample} present the Kokkos programming model and an example application. In Chapter~\ref{chap:backend} we provide insight into a Kokkos backend implementation. Chapters~\ref{chap:related} and ~\ref{chap:teaching} discuss related work, best practices in teaching, and student feedback from tutorials. Lastly Chapter~\ref{chap:conclusion} concludes this work.

